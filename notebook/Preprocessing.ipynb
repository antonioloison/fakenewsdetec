{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = Path('../data/')\n",
    "isot_folder = Path('isot/')\n",
    "kaggle_folder = Path('kaggle/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_isot_dataset(folder: Path):\n",
    "    \"\"\"Load isot dataset.\n",
    "    \n",
    "    :param folder: folder to look for isot files\n",
    "    :type folder: pathlib.Path\n",
    "    \n",
    "    :rtype: pd.Dataframe\n",
    "    :return: isot dataset\"\"\"\n",
    "    isot_df = []\n",
    "    \n",
    "    for label, csv_file in enumerate(['True.csv', 'Fake.csv']):\n",
    "        isot_df.append(pd.read_csv(folder / Path(csv_file), header=0))\n",
    "        isot_df[-1]['label'] = label\n",
    "        \n",
    "\n",
    "    isot_df_concat = pd.concat(isot_df)\n",
    "    isot_df_concat['dataset'] = 'isot'\n",
    "    \n",
    "    isot_df_concat = isot_df_concat.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    return isot_df_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_kaggle_dataset(folder: Path):\n",
    "    \"\"\"Load kaggle dataset.\n",
    "    \n",
    "    :param folder: folder to look for kaggle files\n",
    "    :type folder: pathlib.Path\n",
    "    \n",
    "    :rtype: pd.Dataframe\n",
    "    :return: kaggle dataset\"\"\"\n",
    "    kaggle_df = pd.read_csv(folder / Path('train.csv'), header=0, index_col='id')\n",
    "    kaggle_df['dataset'] = 'kaggle'\n",
    "        \n",
    "    return kaggle_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(df: pd.DataFrame):\n",
    "    \"\"\"Apply preprocessing operations to dataset.\n",
    "    \n",
    "    :param df: dataset\n",
    "    :type dataset: pd.DataFrame\"\"\"\n",
    "    # Remove break lines\n",
    "    df['text'].str.replace('\\n', ' ')\n",
    "    \n",
    "    # Filter articles too short\n",
    "    text_length = df['text'].apply(len)\n",
    "    df = df[text_length > 10]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset: int = 0):\n",
    "    \"\"\"Load the specified dataset.\n",
    "    \n",
    "    :param dataset: 0 loads isot + kaggle; 1 loads isot; 2 loads kaggle\n",
    "    :type dataset: int, optional\n",
    "    \n",
    "    :rtype: pd.Dataframe\n",
    "    :return: dataset\n",
    "    \"\"\"\n",
    "    if dataset==0:\n",
    "        df = pd.concat([load_isot_dataset(data_folder / isot_folder), load_kaggle_dataset(data_folder / kaggle_folder)])\n",
    "        df= df.sample(frac=1).reset_index(drop=True)\n",
    "    elif dataset==1:\n",
    "        df = load_isot_dataset(data_folder / isot_folder)\n",
    "    elif dataset==2:\n",
    "        df = load_kaggle_dataset(data_folder / kaggle_folder)\n",
    "        \n",
    "    df['text'] = df['text'].astype(str)\n",
    "    df['title'] = df['title'].astype(str)\n",
    "\n",
    "    df = preprocess_dataset(df)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_dataset().head(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>this is a test hh</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>this is a test h</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>this is a test h</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>this is a test h</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>this is something different</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>this is something different</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>this is something different</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>this is something different</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>afa</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           text  b\n",
       "1             this is a test hh  1\n",
       "7              this is a test h  2\n",
       "2              this is a test h  3\n",
       "56             this is a test h  4\n",
       "0   this is something different  5\n",
       "65  this is something different  6\n",
       "9   this is something different  7\n",
       "11  this is something different  8\n",
       "3                           afa  9"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "df_test = pd.DataFrame(np.array([\n",
    "    ['this is a test hh', 1],\n",
    "    ['this is a test h', 2],\n",
    "    ['this is a test h', 3],\n",
    "    ['this is a test h', 4],\n",
    "\n",
    "    ['this is something different', 5],\n",
    "    ['this is something different', 6],\n",
    "    ['this is something different', 7],\n",
    "    ['this is something different', 8],\n",
    "    ['afa', 9]\n",
    "\n",
    "]),\n",
    "\n",
    "                   columns=['text', 'b'],\n",
    "                   index=[1,7,2,56,0,65,9,11,3])\n",
    "\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     True\n",
       "1     True\n",
       "2     True\n",
       "3     True\n",
       "4     True\n",
       "5     True\n",
       "6     True\n",
       "7    False\n",
       "8    False\n",
       "Name: a, dtype: bool"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['a'] = pd.Series(df_test.tail(2)['b'])\n",
    "df_test['a'].isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/.local/lib/python3.6/site-packages/ipykernel_launcher.py:15: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>this is a test hh</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>this is something different</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>afa</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          text  b\n",
       "1            this is a test hh  1\n",
       "0  this is something different  5\n",
       "3                          afa  9"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_duplicates(df: pd.DataFrame, field: str, threshold: int = 90):\n",
    "    \"\"\"Remove duplicates from dataframe.\n",
    "    \n",
    "    :param df: dataframe\n",
    "    :type df: pd.DataFrame\n",
    "    :param field: similarity \n",
    "    :type field: str\n",
    "    :param threshold: similarity threshold\n",
    "    :type threshold: float\"\"\"\n",
    "    similarity = pd.Series(index=df.index, dtype=float)\n",
    "    for i, row in enumerate(df.itertuples()):\n",
    "        current_similarity =  df.tail(df.shape[0]-(i+1))[field].apply(lambda val: fuzz.ratio(val, getattr(row, field)))\n",
    "        similarity = pd.concat([similarity, current_similarity], axis=1).max(axis=1)\n",
    "\n",
    "    return df[(similarity < threshold) | (similarity.isnull())]\n",
    "        \n",
    "clean_df = remove_duplicates(df_test, 'text')\n",
    "clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vectorizer(\n",
    "    clean: pd.Series,\n",
    "    analyzer: str = 'char', \n",
    "    ngram_range: Tuple[int, int] = (1, 4), \n",
    "    n_neighbors: int = 1, \n",
    "    **kwargs\n",
    "    ) -> Tuple:\n",
    "    # Create vectorizer\n",
    "    vectorizer = TfidfVectorizer(analyzer = analyzer, ngram_range = ngram_range, **kwargs)\n",
    "    X = vectorizer.fit_transform(clean.values.astype('U'))\n",
    "\n",
    "    # Fit nearest neighbors corpus\n",
    "    nbrs = NearestNeighbors(n_neighbors = n_neighbors, metric = 'cosine').fit(X)\n",
    "    return vectorizer, nbrs\n",
    "\n",
    "#clean -> 'text' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_matches_fuzzy(\n",
    "    row,\n",
    "    match_candidates,\n",
    "    limit = 5\n",
    "    ):\n",
    "    row_matches = process.extract(\n",
    "        row, dict(enumerate(match_candidates)),\n",
    "        scorer = fuzz.token_sort_ratio,\n",
    "        limit = limit\n",
    "        )\n",
    "    result = [(row, match[0], match[1]) for match in row_matches]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuzzy_tf_idf(\n",
    "        df: pd.DataFrame,\n",
    "        column: str,\n",
    "        clean: pd.Series,\n",
    "        mapping_df: pd.DataFrame,\n",
    "        col: str,\n",
    "        analyzer: str = 'char',\n",
    "        ngram_range: Tuple[int, int] = (1, 3)\n",
    "    ) -> pd.Series:\n",
    "    # Create vectorizer\n",
    "    clean = clean.drop_duplicates().reset_index(drop = True)\n",
    "    messy_prep = df[column].drop_duplicates().dropna().reset_index(drop = True).astype(str)\n",
    "    messy = messy_prep.apply(preprocess_string)\n",
    "    result = fuzzy_nn_match(messy = messy, clean = clean, column = column, col = col, n_neighbors = 1)\n",
    "    # Map value from messy to clean\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import module for data manipulation\n",
    "import pandas as pd\n",
    "# Import module for linear algebra\n",
    "import numpy as np\n",
    "# Import module for Fuzzy string matching\n",
    "from fuzzywuzzy import fuzz, process\n",
    "# Import module for regex\n",
    "import re\n",
    "# Import module for iteration\n",
    "import itertools\n",
    "# Import module for function development\n",
    "from typing import Union, List, Tuple\n",
    "# Import module for TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Import module for cosine similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# Import module for KNN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# String pre-processing\n",
    "def preprocess_string(s):\n",
    "    # Remove spaces between strings with one or two letters\n",
    "    s = re.sub(r'(?<=\\b\\w)\\s*[ &]\\s*(?=\\w\\b)', '', s)\n",
    "    return s\n",
    "\n",
    "# String matching - TF-IDF\n",
    "def build_vectorizer(\n",
    "    clean: pd.Series,\n",
    "    analyzer: str = 'char', \n",
    "    ngram_range: Tuple[int, int] = (1, 4), \n",
    "    n_neighbors: int = 1, \n",
    "    **kwargs\n",
    "    ) -> Tuple:\n",
    "    # Create vectorizer\n",
    "    vectorizer = TfidfVectorizer(analyzer = analyzer, ngram_range = ngram_range, **kwargs)\n",
    "    X = vectorizer.fit_transform(clean.values.astype('U'))\n",
    "\n",
    "    # Fit nearest neighbors corpus\n",
    "    nbrs = NearestNeighbors(n_neighbors = n_neighbors, metric = 'cosine').fit(X)\n",
    "    return vectorizer, nbrs\n",
    "\n",
    "# String matching - KNN\n",
    "def tfidf_nn(\n",
    "    messy, \n",
    "    clean, \n",
    "    n_neighbors = 1, \n",
    "    **kwargs\n",
    "    ):\n",
    "    # Fit clean data and transform messy data\n",
    "    vectorizer, nbrs = build_vectorizer(clean, n_neighbors = n_neighbors, **kwargs)\n",
    "    input_vec = vectorizer.transform(messy)\n",
    "\n",
    "    # Determine best possible matches\n",
    "    distances, indices = nbrs.kneighbors(input_vec, n_neighbors = n_neighbors)\n",
    "    nearest_values = np.array(clean)[indices]\n",
    "    return nearest_values, distances\n",
    "\n",
    "# String matching - match fuzzy\n",
    "def find_matches_fuzzy(\n",
    "    row, \n",
    "    match_candidates, \n",
    "    limit = 5\n",
    "    ):\n",
    "    row_matches = process.extract(\n",
    "        row, dict(enumerate(match_candidates)), \n",
    "        scorer = fuzz.token_sort_ratio, \n",
    "        limit = limit\n",
    "        )\n",
    "    result = [(row, match[0], match[1]) for match in row_matches]\n",
    "    return result\n",
    "\n",
    "# String matching - TF-IDF\n",
    "def fuzzy_nn_match(\n",
    "    messy,\n",
    "    clean,\n",
    "    column,\n",
    "    col,\n",
    "    n_neighbors = 100,\n",
    "    limit = 5, **kwargs):\n",
    "    nearest_values, _ = tfidf_nn(messy, clean, n_neighbors, **kwargs)\n",
    "\n",
    "    results = [find_matches_fuzzy(row, nearest_values[i], limit) for i, row in enumerate(messy)]\n",
    "    df = pd.DataFrame(itertools.chain.from_iterable(results),\n",
    "        columns = [column, col, 'Ratio']\n",
    "        )\n",
    "    return df\n",
    "\n",
    "# String matching - Fuzzy\n",
    "def fuzzy_tf_idf(\n",
    "    df: pd.DataFrame,\n",
    "    column: str,\n",
    "    clean: pd.Series,\n",
    "    mapping_df: pd.DataFrame,\n",
    "    col: str,\n",
    "    analyzer: str = 'char',\n",
    "    ngram_range: Tuple[int, int] = (1, 3)\n",
    "    ) -> pd.Series:\n",
    "    # Create vectorizer\n",
    "    clean = clean.drop_duplicates().reset_index(drop = True)\n",
    "    messy_prep = df[column].drop_duplicates().dropna().reset_index(drop = True).astype(str)\n",
    "    messy = messy_prep.apply(preprocess_string)\n",
    "    result = fuzzy_nn_match(messy = messy, clean = clean, column = column, col = col, n_neighbors = 1)\n",
    "    # Map value from messy to clean\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "df_result = (df.pipe(fuzzy_tf_idf, # Function and messy data\n",
    "                     column = 'Expedia', # Messy column in data\n",
    "                     clean = df['Booking.com'], # Master data (list)\n",
    "                     mapping_df = df, # Master data\n",
    "                     col = 'Result') # Can be customized\n",
    "            )\n",
    "end = time.time()# Print the computation time\n",
    "print('Fuzzy string matching in {} seconds'.format(end - start))# View the result of fuzzy string matching\n",
    "df_result.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
